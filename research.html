<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Weidi Xie</title>
  
  <meta name="author" content="Weidi Xie">
  <meta name="viewport" content="width=device-width", initial-scale="1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/svg+xml" href="images/icon.svg">
</head>

<body>
  <table id="container">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <p align="center">
              <a href=index.html>Home</a>&nbsp/&nbsp 
              <a href=about.html>About&nbspMe</a>&nbsp/&nbsp
              <a href=people.html>People</a>&nbsp/&nbsp
              <a href=research.html>Research</a>
            </p>
            <hr>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <heading>Preprint</heading>
                <ol>
                  <li>
                  <a href="https://arxiv.org/abs/2104.07658">
                  <papertitle>Self-supervised Video Object Segmentation by Motion Grouping.</papertitle>
                  </a>
                  <br>
                    Charig Yang, Hala Lamdouar, Erika Lu, Andrew Zisserman, <strong>Weidi Xie</strong>
                  <br>
                    <u><strong>Summary:</strong></u> we propose a self-supervised learning approach for motion segmentation,
                    i.e. foreground object vs. background, which achieves comparable performance as those trained with
                    strong supervision on several popular benchmarks, e.g. DAVIS2016, MoCA (camouflage detection).<br>
                    <a href="https://charigyang.github.io/motiongroup/">Project Page</a> |
                    <a href="https://arxiv.org/abs/2104.06394">Arxiv</a>
                  <p> </p>
                  <li>
                  <a href="https://arxiv.org/abs/2104.06394">
                  <papertitle>All You Need Are a Few Pixels: Semantic Segmentation with PixelPick.</papertitle>
                  </a>
                  <br>
                    Gyungin Shin,  <strong>Weidi Xie</strong>, Samuel Albanie
                  <br>
                    <u><strong>Summary:</strong></u> we investigate efficient annotation for semantic segmentation,
                    and show that, with active learning, only sparse pixel annotation is required to achieve satisfactory results.<br>
                    <a href="https://www.robots.ox.ac.uk/~vgg/research/pixelpick/">Project Page</a> |
                    <a href="https://arxiv.org/abs/2104.06394">Arxiv</a>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2103.14653">
                  <papertitle>Quantum Self-supervised Learning.</papertitle>
                  </a>
                  <br>
                    Ben Jaderberg, Lewis W. Anderson, <strong>Weidi Xie</strong>, Samuel Albanie, Martin Kiffner, Dieter Jaksch
                  <br>
                    <u><strong>Summary:</strong></u> to open up the concept of QNNs for self-supervised learning for computer vision task,
                    lots of work remains to be done, to further show its scalability on real devices.<br>
                    <a href="https://arxiv.org/abs/2103.14653">Arxiv</a>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2102.07064">
                  <papertitle>NeRF--: Neural Radiance Fields Without Known Camera Parameters.</papertitle>
                  </a>
                  <br>
                    Zirui Wang, Shangzhe Wu,  <strong>Weidi Xie</strong>, Min Chen, Victor Adrian Prisacariu
                  <br>
                    <u><strong>Summary:</strong></u> we show the possibility of jointly optimizing camera parameters and neural radiance fields.<br>
                  <a href="https://nerfmm.active.vision">Project Page</a> |
                  <a href="https://arxiv.org/abs/2102.07064">Arxiv</a>
                </li>
                  <p> </p>


                </ol>


                <heading>2021</heading>
                <ol>
                  <li>
                    <a href="--">
                    <papertitle>Localizing Visual Sounds the Hard Way.</papertitle></a>
                    <br>
                    Honglie Chen, <b>Weidi Xie</b>, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, Andrew Zisserman
                    <br>
                    In: <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2021
                    <br>
                    <a href="https://www.robots.ox.ac.uk/~vgg/research/lvs/">Project Page</a> |
                    <a href="https://arxiv.org/abs/2104.02691">Arxiv</a> </li>
                    <p> </p>
                  <li>
                    <a href="https://www.sciencedirect.com/science/article/pii/S136184152100044X#!">
                    <papertitle>Learning to Map 2D Ultrasound Images into 3D Space with Minimal Human Annotation.</papertitle></a>
                    <br>
                    Pak-Hei Yeung, Moska Aliasi, Aris T. Papageorghiou, Monique Haak, <b>Weidi Xie</b>, Ana I.L. Namburete.
                    <br>
                    In: <em>Medical Image Analysis, February 2021. (Impact Factor: ~11)</em>
                    <br>
                    <a href="https://www.sciencedirect.com/science/article/pii/S136184152100044X#!"> Paper</a></li>
                    <p> </p>
                </ol>

                <heading>2020</heading>
                <ol>
                  <li>
                    <a href="https://arxiv.org/pdf/2012.06867.pdf">
                    <papertitle>VoxSRC 2020: The Second VoxCeleb Speaker Recognition Challenge.</papertitle>
                    </a>
                    <br>
                    Arsha Nagrani,
                    Joon Son Chung,
                    Jaesung Huh,
                    Andrew Brown,
                    Ernesto Coto,
                    <strong>Weidi Xie</strong>,
                    Mitchell McLaren,
                    Douglas A Reynolds,
                    Andrew Zisserman.<br>
                    <a href="https://www.sciencedirect.com/science/article/pii/S136184152100044X#!"> Tech Report</a></li>
                    <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2010.09709">
                  <papertitle>Self-supervised Co-training for Video Representation Learning.</papertitle>
                  </a>
                  <br>
                  Tengda Han,
                  <strong>Weidi Xie</strong>, Andrew Zisserman <br>
                  In: <em>Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS) </em>, 2020. <br>
                  <a href="https://arxiv.org/abs/2010.09709">Arxiv</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/research/CoCLR/">Project Page</a> |
                  <a href="https://github.com/TengdaHan/CoCLR">Code & Model</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2011.11630">
                  <papertitle>Betrayed by Motion: Camouflaged Object Discovery via Motion Segmentation.</papertitle>
                  </a>
                  <br>
                  Hala Lamdouar, Charig Yang, <strong>Weidi Xie</strong>, Andrew Zisserman<br>
                  In: <em>Asian Conference on Computer Vision (ACCV)</em>, 2020. <br>
                  <a href="https://arxiv.org/abs/2011.11630">Arxiv</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2020/Lamdouar20/lamdouar20.pdf">PDF</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/data/MoCA/">Project Page</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2009.07833">
                  <papertitle>Layered Neural Rendering for Retiming People in Video.</papertitle>
                  </a>
                  <br>
                  Erika Lu, Forrester Cole, Tali Dekel, <strong>Weidi Xie</strong>, Andrew Zisserman, David Salesin, William T. Freeman, Michael Rubinstein<br>
                  In: <em>ACM Transactions on Graphics (TOG). Proc. SIGGRAPH Asia </em>, 2020<br>

                  <a href="https://arxiv.org/abs/2009.07833">Arxiv</a> |
                  <a href="https://retiming.github.io">Project Page</a></li>
                  <p> </p>

                  <li>
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2020/Xie20/xie20.pdf">
                  <papertitle>Inducing Predictive Uncertainty Estimation for Face Recognition.</papertitle>
                  </a>
                  <br>
                  <strong>Weidi Xie</strong>, Jeffrey Byrne, Andrew Zisserman<br>
                  In: <em>British Machine Vision Conference (BMVC) </em>, 2020<br>
                  <a href="https://arxiv.org/abs/2009.00603">Arxiv</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2020/Xie20/xie20.pdf">PDF</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2007.12163">
                  <papertitle>Smooth-AP: Smoothing the Path Towards Large-Scale Image Retrieval.</papertitle>
                  </a>
                  <br>
                  Andrew Brown, <strong>Weidi Xie</strong>, Vicky Kalogeiton, Andrew Zisserman<br>
                  In: <em>European Conference on Computer Vision (ECCV) </em>, 2020<br>
                   <a href="https://arxiv.org/abs/2007.12163">Arxiv</a> |
                   <a href="https://www.robots.ox.ac.uk/~vgg/research/smooth-ap/">Project Page</a> |
                  <a href="https://github.com/Andrew-Brown1/Smooth_AP">Code & Model</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2008.01065">
                  <papertitle>Memory-augmented Dense Predictive Coding for Video Representation Learning.</papertitle>
                  </a>
                  <br>
                  Tengda Han,
                  <strong>Weidi Xie</strong>, Andrew Zisserman <br>
                  In: <em>European Conference on Computer Vision (ECCV) </em>, 2020
                  &nbsp <font color="red"><strong>(Spotlight Presentation)</strong></font> <br>
                  <a href="https://arxiv.org/abs/2008.01065">Arxiv</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/research/DPC/">Project Page</a> |
                  <a href="https://tengdahan.github.io">Code & Model</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2002.07793">
                  <papertitle>MAST: A Memory-Augmented Self-Supervised Tracker.</papertitle>
                  </a>
                  <br>
                  Zihang Lai,
                  Erika Lu,
                  <strong>Weidi Xie</strong> <br>
                  In: <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020<br>
                  <a href="https://arxiv.org/abs/2002.07793">Arxiv</a> |
                  <a href="https://zlai0.github.io/MAST/">Project Page</a> |
                  <a href="https://github.com/zlai0/MAST">Code & Model</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/2004.14368">
                  <papertitle>VGG-Sound: A Large-Scale Audio-Visual Dataset.</papertitle>
                  </a>
                  <br>
                  Honglie Chen,
                  <strong>Weidi Xie</strong>,
                  Andrea Vedaldi,
                  Andrew Zisserman <br>
                  In: <em>International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2020<br>
                  <a href="https://arxiv.org/abs/2004.14368">Arxiv</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2020/Chen20/chen20.pdf">PDF</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/data/vggsound/">Project Page</a> |
                  <a href="https://github.com/hche11/VGGSound">Code & Model</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://ieeexplore.ieee.org/document/8999615">
                  <papertitle>Low-Memory CNNs Enabling Real-Time Ultrasound Segmentation Towards Mobile Deployment.</papertitle>
                  </a>
                  <br>
                  Sagar Vaze, <strong>Weidi Xie</strong>, Ana Namburete. <br>
                  In: <em>IEEE Journal of Biomedical and Health Informatics</em>, 2020. (Impact Factor: ~4.2)<br>
                  <a href="https://sgvaze.github.io/pages/lightweight_unets.html">Project Page</a>  |
                  <a href="https://github.com/sgvaze/lightweight_unet">Code</a></li>
                  <p> </p>

                  <li>
                  <a href="https://www.sciencedirect.com/science/article/pii/S0885230819302712">
                  <papertitle>VoxCeleb: Large-scale Speaker Verification in the Wild.</papertitle>
                  </a>
                  <br>
                  Arsha Nagrani*, Joon Son Chung*, <strong>Weidi Xie*</strong>,
                  Andrew Zisserman.  (* indicates equal contribution)<br>
                  In: <em>Computer Speech & Language</em>, 2020. (Impact Factor: ~1.8)<br></li>
                  <p> </p>
                </ol>


                <heading>2019</heading>
                <ol>
                  <li>
                  <a href="https://arxiv.org/pdf/1912.02522.pdf">
                  <papertitle>VoxSRC 2019: The first VoxCeleb Speaker Recognition Challenge.</papertitle>
                  </a>
                  <br>
                  Joon Son Chung, Arsha Nagrani,
                  Ernesto Coto,
                  <strong>Weidi Xie</strong>,
                  Mitchell McLaren, Douglas A Reynolds,
                  Andrew Zisserman.<br>
                  <a href="https://arxiv.org/pdf/1912.02522.pdf">Tech Report</a></li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/1909.04656">
                  <papertitle>Video Representation Learning by Dense Predictive Coding.</papertitle>
                  </a>
                  <br> Tengda Han,
                  <strong>Weidi Xie</strong>,
                  Andrew Zisserman<br>
                  In: <em>1st International Workshop on Large-scale Holistic Video Understanding, ICCV</em>, 2019.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> <br>
                  <a href="https://arxiv.org/abs/1909.04656">Arxiv</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/research/DPC/">Project Page</a> |
                  <a href="https://github.com/TengdaHan/DPC">Code</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/1905.00875">
                  <papertitle>Self-supervised Learning for Video Correspondence Flow.</papertitle>
                  </a>
                  <br> Zihang Lai,
                  <strong>Weidi Xie</strong> <br>
                  In: <em>British Machine Vision Conference (BMVC)</em>, 2019.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> <br>
                  <a href="https://arxiv.org/abs/1905.00875">Arxiv</a> |
                  <a href="https://zlai0.github.io/CorrFlow/">Project Page</a> </li>
                  <p> </p>

                  <li>
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Chen19/chen19.pdf">
                  <papertitle>AutoCorrect: Deep Inductive Alignment of Noisy Geometric Annotations.</papertitle>
                  </a>
                  <br> Honglie Chen,
                  <strong>Weidi Xie</strong>,
                  <a href="http://www.robots.ox.ac.uk/~vedaldi/"> Andrea Vedaldi</a>,
                  Andrew Zisserman. <br>
                  In: <em>British Machine Vision Conference (BMVC)</em>, 2019.
                  &nbsp <font color="red"><strong>(Spotlight Presentation)</strong></font> <br>
                  <a href="https://arxiv.org/abs/1908.05263">Arxiv</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Chen19/chen19.pdf">PDF</a> </li>
                  <p> </p>

                  <li>
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/xu19/xu19.pdf">
                  <papertitle>Geometry-Aware Corner Network for Video Object Detection from Static Cameras.</papertitle>
                  </a>
                  <br> Dan Xu,
                  <strong>Weidi Xie</strong>,
                  Andrew Zisserman. <br>
                  In: <em>British Machine Vision Conference (BMVC)</em>, 2019.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> <br>
                  <a href="https://arxiv.org/abs/1909.03140">Arxiv</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/xu19/xu19.pdf">PDF</a> </li>
                  <p> </p>

                  <li>
                  <a href="https://arxiv.org/abs/1902.10107">
                  <papertitle>Utterance-level Aggregation for Speaker Recognition in the Wild.</papertitle>
                  </a>
                  <br>
                  <strong>Weidi Xie</strong>,
                  Arsha Nagrani, Joon Son Chung, Andrew Zisserman. <br>
                  In: <em>International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</em>, 2019.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> <br>
                  <a href="https://arxiv.org/abs/1902.10107">Arxiv</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/research/speakerID/">Project Page</a> |
                  <a href="https://github.com/WeidiXie/VGG-Speaker-Recognition">Code & Model</a></li>
                  <p> </p>
                </ol>


                <heading>2018</heading>
                <ol>
                  <li>
                  <a href="https://www.robots.ox.ac.uk/~vgg/publications/2018/Xie18a/xie18a.pdf">
                  <papertitle>Comparator Networks.</papertitle>
                  </a>
                  <br>
                  <strong>Weidi Xie</strong>, Li Shen, Andrew Zisserman
                  <br>
                  In: <em>European Conference on Computer Vision (ECCV)</em>, 2018.
                  <br>
                  <a href="https://arxiv.org/abs/1807.11440">Arxiv</a> |
                  <a href="https://www.robots.ox.ac.uk/~vgg/publications/2018/Xie18a/xie18a.pdf">PDF</a></li>
                  <p> </p>

                  <li>
                  <a href="https://www.robots.ox.ac.uk/~vgg/publications/2018/Xie18b/xie18b.pdf">
                  <papertitle>Multicolumn Networks on Face Recognition.</papertitle>
                  </a>
                  <br>
                  <strong>Weidi Xie</strong>, Andrew Zisserman
                  <br>
                  In: <em>British Machine Vision Conference (BMVC)</em>, 2018.
                  <br>
                  <a href="https://arxiv.org/abs/1807.09192">Arxiv</a> |
                  <a href="https://www.robots.ox.ac.uk/~vgg/publications/2018/Xie18b/xie18b.pdf">PDF</a> |
                  <a href="https://github.com/WeidiXie/multicoumn_network">Code & Model</a> |
                  <a href="data/XieBMVC2018.bib">Bibtex</a></li>
                  <p> </p>

                  <li>
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Lu18/lu18.pdf">
                  <papertitle>Class-Agnostic Counting.</papertitle>
                  </a>
                  <br>
                  Erika Lu, <strong>Weidi Xie</strong>, Andrew Zisserman
                  <br>
                  In: <em>Asian Conference on Computer Vision (ACCV)</em>, 2018.
                  <br>
                  <a href="https://arxiv.org/abs/1811.00472">Arxiv</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/research/class-agnostic-counting/">Project Page</a> |
                  <a href="data/XieBMVC2018.bib">Bibtex</a></li>
                  <p> </p>

                  <li>
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Cao18/cao18.pdf">
                  <papertitle>VGGFace2: A Dataset for Recognising Faces Across Pose and Age.</papertitle>
                  </a>
                  <br>
                  Qiong Cao, Li Shen, <strong>Weidi Xie</strong>, Omkar M. Parkhi and Andrew Zisserman
                  <br>
                  In: <em>IEEE International Conference on Automatic Face and Gesture Recognition (F&G)</em>, 2018.
                  &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/1710.08092">Arxiv</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Cao18/cao18.pdf">PDF</a> |
                  <a href="http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/">Project Page</a> |
                  <a href="data/CaoFG2018.bib">Bibtex</a></li>
                  <p> </p>

                  <li>
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518302998">
                  <papertitle>Omega-Net: Fully Automatic, Multi-View Cardiac MR Detection, Orientation, and Segmentation with Deep Neural Networks.</papertitle>
                  </a>
                  <br>
                  <strong>Weidi Xie*, Davis M. Vigneault*</strong>, Carolyn Y. Ho, David A. Bluemke and J. Alison Noble (*joint first author)
                  <br>
                  In: <em>Medical Image Analysis, Volume 48, Pages 95, August 2018. (Impact Factor: ~11)</em>
                  <br>
                  <a href="https://arxiv.org/abs/1711.01094">Arxiv</a> |
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518302998">Paper</a></li>
                  <p> </p>

                  <li>
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518301920">
                  <papertitle>VP-Nets: Efficient Automatic Localization of Key Brain Structures in 3D Fetal Neurosonography.</papertitle>
                  </a>
                  <br>
                  Ruobing Huang, <strong>Weidi Xie</strong> and J. Alison Noble
                  <br>
                  In: <em>Medical Image Analysis, Volume 47, Pages 127, July 2018. (Impact Factor: ~11)</em>
                  <br>
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518301920">Paper</a></li>
                  <p> </p>

                  <li>
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518301920">
                  <papertitle>VP-Nets: Efficient Automatic Localization of Key Brain Structures in 3D Fetal Neurosonography.</papertitle>
                  </a>
                  <br>
                  Ruobing Huang, <strong>Weidi Xie</strong> and J. Alison Noble
                  <br>
                  In: <em>Medical Image Analysis, Volume 47, Pages 127, July 2018. (Impact Factor: ~11)</em>
                  <br>
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518301920">Paper</a></li>
                  <p> </p>

                  <li>
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518300306">
                  <papertitle>Fully-Automated Alignment of 3D Fetal Brain Ultrasound to a Canonical Reference Space Using Multi-task Learning.</papertitle>
                  </a>
                  <br>
                  <strong>Weidi Xie*, Ana I.L. Namburete*</strong>,   Mohammad Yaqub,
                  Andrew Zisserman and J. Alison Noble (*joint first author)
                  <br>
                  In: <em>Medical Image Analysis, Volume 46, Pages 1, May 2018. (Impact Factor: ~11)</em>
                  <br>
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518300306">Paper</a></li>
                  <p> </p>
                </ol>

                <heading>2017</heading>
                <ol>
                  <li>
                  <a href="https://link.springer.com/chapter/10.1007/978-3-319-59448-4_18">
                  <papertitle>Feature Tracking Cardiac Magnetic Resonance via Deep Learning and Spline Optimization.</papertitle>
                  </a>
                  <br>
                  Davis M. Vigneaulta, <strong>Weidi Xie</strong>, David A. Bluemke and J. Alison Noble
                  <br>
                  In: <em>Functional Imaging and Modelling of the Heart (FIMH)</em>, 2017.
                  &nbsp <font color="red"><strong>(Best Poster Award)</strong></font>
                  <br>
                  <a href="https://arxiv.org/abs/1704.03660">Arxiv</a> |
                  <a href="https://link.springer.com/chapter/10.1007/978-3-319-59448-4_18">Paper</a></li>
                  <p> </p>

                  <li>
                  <a href="https://link.springer.com/chapter/10.1007%2F978-3-319-67561-9_8">
                  <papertitle>Robust Regression of Brain Maturation from 3D Fetal Neurosonography using CRNs.</papertitle>
                  </a>
                  <br>
                  Ana I.L. Namburete, <strong>Weidi Xie</strong> and J. Alison Noble
                  <br>
                  In: <em>MICCAI Workshop on Fetal and InFant Image analysis (FIFI)</em>, 2017.
                  &nbsp <font color="red"><strong>(Best Paper Award)</strong></font>
                  <br>
                  <a href="https://www.dropbox.com/s/ypyita3gabr2cs4/3d_brain_age.pdf?dl=0">Paper</a></li>
                  <p> </p>
                </ol>

                <heading>2016</heading>
                <ol>
                  <li>
                  <a href="https://www.tandfonline.com/doi/full/10.1080/21681163.2016.1149104">
                  <papertitle>Microscopy Cell Counting and Detection with Fully Convolutional Regression Networks.</papertitle>
                  </a>
                  <br>
                  <strong>Weidi Xie</strong>, J. Alison Noble and Andrew Zisserman
                  <br>
                  In: <em>MICCAI 1st Deep Learning Workshop</em>, 2015.
                  <br>
                  In: <em>Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization</em>, 2016.
                  &nbsp <font color="red"><strong>(Biannual Best Journal Article)</strong></font>
                  <br>
                  <a href="http://www.robots.ox.ac.uk/~vgg/publications/2016/Xie16/xie16.pdf">Paper</a> |
                  <a href="https://github.com/WeidiXie/cell_counting_v2">Code</a> |
                  <a href="https://think.taylorandfrancis.com/journal-prize-computer-methods-in-biomechanics-and-biomedical-engineering-imaging-visualization-best-paper-award/">Award</a></li>
                  <p> </p>
                </ol>
              </td>
            </tr>
            </table>
            <!--<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <td width="25%">
                        <img src="images/SILVER.gif">
                      </td>
                      <td valign="middle" width="75%">
                        
                          <papertitle>SILVER - an Improvement upon Radial Golden Ratio Sampling</papertitle>
                        
                        <br>
                        <strong>S. Sophie Schauman</strong>, 
                        <a href="https://www.ndcn.ox.ac.uk/team/thomas-okell">Thomas W. Okell</a>,
                        <a href="https://www.ndcn.ox.ac.uk/team/mark-chiew">Mark Chiew</a> 
                        <br>
                        <em>Submitted to the ISMRM Workshop on Sampling and Image Reconstruction, Sedona, USA</em>, 2020
                        <br>
                  <a href="https://github.com/SophieSchau/SILVER">GitHub</a>, <a href="data/posters/ISMRM_WS_SEDONA2020.pdf">Poster</a>
                        <br>
                        <p></p>
                        <p>Radial sampling in MRI has many advantages over Cartesian trajectories, including less coherent aliasing when undersampled and robustness to motion. Uniform radial sampling provides the highest SNR efficiency but lacks flexibility in choosing temporal windows for reconstruction. Golden means sampling, on the other hand, provide near-optimal efficiency for arbitrary window sizes. The Golden means method is based on doing a set increment that fills k-space as uniformly as possible. A constant increment is beneficial as it makes the efficiency of the reconstruction shift-invariant, allowing for flexible reconstruction windows starting from any spoke.
          We aim to show that by relaxing the requirement to have close to uniform sampling for any window size, by instead optimizing for a restricted window size range of interest, higher efficiency can be achieved by using different increments. This is a reasonable limitation, as meaningful images are never reconstructed from just one or a handful of spokes and uniformity is not important once fully sampled. We call this the Set Increment with Limited Views Enhancing Ratio (SILVER) method.
          </p>
                      </td>
                    </tr>
          
          
                    <tr>
                      <td width="25%">
                        <img src="images/ISMRM2019.gif">
                      </td>
                      <td valign="middle" width="75%">
                        
                          <papertitle>Accelerated Vessel-Encoded Arterial Spin Labeling Angiography</papertitle>
                        
                        <br>
                        <strong>S. Sophie Schauman</strong>, 
                        <a href="https://www.ndcn.ox.ac.uk/team/mark-chiew">Mark Chiew</a>,
                        <a href="https://www.ndcn.ox.ac.uk/team/thomas-okell">Thomas W. Okell</a> 
                        <br>
                        <em>ISMRM Annual Meeting, Montreal, Canada</em>, 2019
                        <br>
                      <a href="hhttps://cds.ismrm.org/protected/19MPresentations/abstracts/0744.html">abstract</a>  (login needed)
                        <br>
                        <p></p>
                        <p>Vessel-encoded ASL can produce vessel-selective cerebral angiograms, but to separate blood from multiple arteries more images are needed than for standard ASL angiography, which increases scan time. Angiograms are however well suited for under-sampling and compressed sensing reconstruction because of their high intrinsic sparsity. In this work we demonstrate in-vivo that vessel-selective angiograms allow for higher acceleration factors, yielding comparable image quality to conventional angiography with matched scan time using 2D and 3D time-resolved golden angle radial acquisitions. With this optimised acquisition and reconstruction method, scan time of the 3D case can be reduced from 8:35 hours to ~5 minutes.</p>
                      </td>
                    </tr>
          
          <tr>
                      <td width="25%">
                        <img src="images/KSPACESIMU.gif">
                      </td>
                      <td valign="middle" width="75%">
                        
                          <papertitle>K-Space Simulator for Public Engagement</papertitle>
                        
                        <br>
                        <strong>S. Sophie Schauman</strong>, 
                        <a href="https://www.ndcn.ox.ac.uk/team/benjamin-tendler">Benjamin Tendler</a>,
                        <a href="https://www.ndcn.ox.ac.uk/team/stuart-clare">Stuart Clare</a> 
                        <br>
                        <em>Wellcome Centre for Integrative Neuroimaging (The Big Brain Roadshow)</em>, 2019
                        <br>
                  <a href="">Medium</a>, <a href="https://github.com/SophieSchau/TheImagedBrain">GitHub</a>
                        <br>
                        <p></p>
                        <p><i>The Big Brain Roadshow</i> is part of the Public Engagement work done at WIN. As Public Engagement Ambassador, one of my projects was to create a stall for 13-16-year-old children to learn about some aspect of the work physicists do in neuroscience labs. We came up with a way of showing the children that any image can be built up of waves (Fourier basis). The tool we built is a Matlab script that can process images shown to a webcam in real-time and produce the 2D Fourier transform of that image. The simulator also works in reverse. If you show the camera an image of k-space, with the real component in one colour channel and the imaginary component in another, it shows the image that that k-space represents without changing any settings.</p>
                      </td>
                    </tr>
          </table>-->
          
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                      <td>
                        <br>
                        <p align="center">
                          <font size="2">
                            Based on a template by <a href="https://jonbarron.info">Jon Barron</a>
                            </font>
                        </p>
                      </td>
                    </tr>
                  </table>
                  </td>
              </tr>
            </table>
          </body>
          
          </html>