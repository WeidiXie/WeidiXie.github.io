<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Weidi Xie</title>
  
  <meta name="author" content="Weidi Xie">
  <meta name="viewport" content="width=device-width", initial-scale="1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/svg+xml" href="images/icon.svg">
</head>

<body>
  <table id="container">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <p align="center">
              <a href=index.html>Home</a>&nbsp/&nbsp 
              <a href=about.html>About&nbspMe</a>&nbsp/&nbsp
              <a href=people.html>People</a>&nbsp/&nbsp
              <a href=research.html>Research</a>
            </p>
            <hr>
          </tr>
        </table>
      </td>
    </tr>
    <tr>
      <td>
        <br>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="40%" valign="center">
              <img src="images/weidi_photo.png" width="220" height="220">
            </td>
            <td width="80%" valign="middle">
              <p align="center">
                <name>Weidi Xie<span style="font-family:STFangsong; font-size:20pt"> (谢伟迪)</span></name> 
              <br>
              <p align="left">
                 I'm joining <a href="https://en.sjtu.edu.cn">Shanghai Jiao Tong University</a> as an Associate Professor.<br> <br>


                 I'll keep the affiliation with <a href="http://www.robots.ox.ac.uk/~vgg/">Visual Geometry Group (VGG)</a> at Oxford,
                 where I have spent seven wonderful years, completing DPhil and working as a Senior Research Fellow afterwards.<br> <br>
                 I was fortunate to have been supervised by 
                 <a href="https://scholar.google.com/citations?user=UZ5wscMAAAAJ&hl=en">Professor Andrew Zisserman</a>
                 and Prof. <a href="https://scholar.google.co.uk/citations?user=b0tmmYMAAAAJ&hl=en">Professor Alison Noble</a>. 
              </p>
              <p>
                <a href="mailto:weidi@robots.ox.ac.uk"> Email </a>&nbsp/&nbsp
                <a href="https://scholar.google.co.uk/citations?user=Vtrqj4gAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/WeidiXie"> Twitter </a> &nbsp/&nbsp
                <a href="https://space.bilibili.com/626918756"> Bilibili </a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/weidi-xie-81a59976/"> LinkedIn </a>
              </p>
              <p> </p>
            </td>
          </tr>
          
        </table>
        <table width="120%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <br>
                <heading>Research</heading>   <br> <br>
                I'm generally interested in understanding how visual perception emerges <strong>without</strong> manual annotations.
                Specifically, learning visual representations from self-supervised training, 
                multi-modal co-training, for example, apperance, motion, audio, textual description,
                and Sim2Real training, i.e. learning from simulator, and generalise to real data.
              </td>
          </tr>
        </table>
        <table width="120%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <br>
              <heading>News</heading>
              <ul>
                <li><b> Dec 2021, </b> <a href="https://ju-chen.github.io/efficient-prompt/"> Prompting Visual-Language Models for Efficient Video Understanding.</a> <em>Preprint</em>.
                <br> <br>
                <li><b> Dec 2021, </b> <a href="https://charigyang.github.io/abouttime/"> It's About Time: Analog Clock Reading in the Wild.</a> <em>Preprint</em>.<br> <br>
                <li><b> Oct 2021, </b> <a href="https://www.robots.ox.ac.uk/~vgg/research/simo/"> Segmenting Invisible Moving Objects.</a>  <em>BMVC2021</em>.<br> <br>
                <li><b> Oct 2021, </b> <a href="https://www.robots.ox.ac.uk/~vgg/research/avs/"> Audio-Visual Synchronisation In The Wild.</a> <em>BMVC2021</em>.<br> <br>
                <li><b> Oct 2021, </b> <a href="https://www.robots.ox.ac.uk/~vgg/research/pixelpick/"> All You Need Are a Few Pixels: Semantic Segmentation with PixelPick.</a>
                <em>ICCV2021, ILDAV Workshop</em>, &nbsp <font color="red"><strong>(Best Paper Award)</strong></font> </a> <br> <br>
                <li><b> Sep 2021, </b> <a href="https://pakheiyeung.github.io/ImplicitVol_wp/"> ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation.</a> <em>Preprint</em> <br> <br>
                <li><b> Sep 2021, </b> <a href="https://xiaoman-zhang.github.io/Layer-Decomposition/"> Self-supervised Tumor Segmentation through Layer Decomposition.</a> <em>Preprint</em> <br> <br>
                <li><b> July 2021, </b> <a href="https://charigyang.github.io/motiongroup/"> Self-supervised Video Object Segmentation by Motion Grouping.</a>
                <em>ICCV2021</em>. &nbsp <font color="red"><strong>(Best Paper Award at CVPR Workshop)</strong></font> </a> <br> <br>
                <li><b> June 2021, </b> KeyNote at
                  <a href="https://eval.vision.rwth-aachen.de/rvsu-workshop21/"> CVPR2021 Robust Video Scene Understanding: Tracking and Video Segmentation.</a><br> <br>
                <li><b> June 2021, </b> <a href="https://pakheiyeung.github.io/Sli2Vol_wp/"> Sli2Vol: Annotate a 3D Volume from a Single Slice with Self-Supervised Learning.</a>
                <em>MICCAI2021</em>.<br> <br>
                <li><b> April 2021, </b> <a href="https://nerfmm.active.vision"> NeRF--: Neural Radiance Fields Without Known Camera Parameters.</a> <em>Paper & Code</em> </a> <br> <br>
                <li><b> April 2021, </b> <a href="https://www.robots.ox.ac.uk/~vgg/research/lvs/"> Localizing Visual Sounds the Hard Way.</a> <em> Paper & Code, CVPR2021</em> </a><br> <br>
                <li><b> March 2021, </b> <a href="https://arxiv.org/abs/2103.14653"> Quantum Self-supervised Learning.</a> <em>Preprint</em>  </a><br> <br>
              </ul>
            </td>
          </tr>
          </table>
                  </td>
              </tr>
            </table>
          
          </body>
          
          </html>
