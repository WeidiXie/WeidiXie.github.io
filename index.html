<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Weidi Xie</title>
  
  <meta name="author" content="Weidi Xie">
  <meta name="viewport" content="width=device-width", initial-scale="1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/svg+xml" href="images/icon.svg">
</head>

<body>
  <table id="container">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <p align="center">
              <a href=index.html>Home</a>&nbsp/&nbsp 
              <a href=about.html>About&nbspMe</a>&nbsp/&nbsp
              <a href=people.html>People</a>&nbsp/&nbsp
              <a href=research.html>Research</a>
            </p>
            <hr>
          </tr>
        </table>
      </td>
    </tr>
    <tr>
      <td>
        <br>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="40%" valign="center">
              <img src="images/weidi_photo.png" width="220" height="220">
            </td>
            <td width="80%" valign="middle">
              <p align="center">
                <name>Weidi Xie<span style="font-family:STFangsong; font-size:20pt"> (谢伟迪)</span></name> 
              <br>
              <p align="left">
                 I'm joining <a href="https://en.sjtu.edu.cn">Shanghai Jiao Tong University</a> as an Associate Professor.<br> <br>


                 I'll remain a visitor researcher with <a href="http://www.robots.ox.ac.uk/~vgg/">Visual Geometry Group (VGG)</a> at Oxford,
                 where I have spent seven wonderful years, completed DPhil and worked as a Senior Research Fellow afterwards.<br> <br>
                 I was fortunate to have been supervised by 
                 <a href="https://scholar.google.com/citations?user=UZ5wscMAAAAJ&hl=en">Professor Andrew Zisserman</a>
                 and <a href="https://scholar.google.co.uk/citations?user=b0tmmYMAAAAJ&hl=en">Professor Alison Noble</a>. 

              </p>
              <p>
                <a href="mailto:weidi@sjtu.edu.cn"> Email </a>&nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Vtrqj4gAAAAJ&hl=zh-CN">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/WeidiXie"> Twitter </a> &nbsp/&nbsp
                <a href="https://space.bilibili.com/626918756"> Bilibili </a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/weidi-xie-81a59976/"> LinkedIn </a>
              </p>
              <p> </p>
            </td>
          </tr>
        </table>

        <table width="120%" align="center" border="0" cellspacing="0" cellpadding="8">
          <tr>
            <td width="100%" valign="middle">
              <br>
                <heading>Research: </heading>   <br> <br>
                I'm generally interested in understanding how visual perception emerges. 
                In particular, on topics:  <br> <br>
                  <li> learning visual representations from self-supervised training. <br> <br>
                  <li> multi-modal co-training with visual apperance, motion, audio, textual description, etc. <br> <br>
                  <li> open-world, object-centric representation learning.<br> <br>                  
                  <li> learning visual representation for embodied agents. 
              </td>
          </tr>
        </table>
        
        <table width="120%" align="center" border="0" cellspacing="0" cellpadding="8">
          <tr>
            <td width="100%" valign="middle">
              <br>
                <heading>To Prospective Student: </heading>   <br> <br>
                If you are enthusiastic to work with me on the above topics, please drop me an email with a <b>CV</b> and <b>Research Proposal</b>.
              </td>
          </tr>
        </table>

        <table width="120%" align="center" border="0" cellspacing="0" cellpadding="8">
          <tr>
            <td width="100%" valign="middle">
              <br>
              <heading>News:</heading> <br> <br>
              <li><b> July 2022, </b> <a href="https://arxiv.org/abs/2207.02206"> Segmenting Moving Objects via an Object-Centric Layered Representation.</a> <em>Preprint</em>. <br> <br>
              <li><b> July 2022, </b> <a href="https://ju-chen.github.io/efficient-prompt/"> Prompting Visual-Language Models for Efficient Video Understanding.</a> <em>To appear at ECCV2022</em>. <br> <br>
              <li><b> July 2022, </b> <a href="https://fcjian.github.io/promptdet"> PromptDet: Towards Open-vocabulary Detection using Uncurated Images.</a> <em>To appear at ECCV2022</em>. <br> <br>
              <li><b> June 2022, </b> <a href="https://arxiv.org/abs/2206.12772">Exploiting Transformation Invariance and Equivariance for Self-supervised Sound Localisation.</a> <em> To appear at ACM MM 2022</em>  </a><br> <br>
              <li><b> June 2022, </b> <a href="https://arxiv.org/abs/2206.06947"> K-Space Transformer for Fast MRI Reconstruction with Implicit Representation.</a> <em> Preprint </em>  </a><br> <br>
              <li><b> June 2022, </b> <a href=""> Adaptive 3D Localization of 2D Freehand Ultrasound Brain Images.</a> <em> To appear at MICCAI2022 </em>  </a><br> <br>
                <li><b> May 2022, </b> <a href=""> Transforming the Interactive Segmentation for Medical Imaging.</a> <em> To appear at MICCAI2022 </em>  &nbsp <font color="red"><strong>(Early Accept)</strong></font> </a> <br> <br>
                <li><b> Apr 2022, </b> <a href="https://arxiv.org/abs/2103.14653"> Quantum Self-supervised Learning.</a> <em> Accepted by Quantum Science and Technology </em>  </a><br> <br>
                <li><b> Mar 2022, </b> <a href="https://arxiv.org/pdf/2203.12614.pdf"> Unsupervised Salient Object Detection with Spectral Cluster Voting.</a> <em> CVPR2022 Workshop</em> <br> <br>
                <li><b> Dec 2021, </b> <a href=""> Temporal Alignment Networks for Long-term Video.</a> <em> CVPR2022</em>. <br> <br>
                <li><b> Dec 2021, </b> <a href="https://arxiv.org/abs/2112.05749v1"> Label, Verify, Correct: A Simple Few Shot Object Detection Method.</a> <em> CVPR2022</em>. <br> <br>
                <li><b> Dec 2021, </b> <a href="https://charigyang.github.io/abouttime/"> It's About Time: Analog Clock Reading in the Wild.</a> <em>CVPR2022</em>.<br> <br>
                <li><b> Oct 2021, </b> <a href="https://www.robots.ox.ac.uk/~vgg/research/pixelpick/"> All You Need Are a Few Pixels: Semantic Segmentation with PixelPick.</a>
                <em>ICCV2021, ILDAV Workshop</em>, &nbsp <font color="red"><strong>(Best Paper Award)</strong></font> </a> <br> <br>
                <li><b> Sep 2021, </b> <a href="https://pakheiyeung.github.io/ImplicitVol_wp/"> ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation.</a> <em>Preprint</em> <br> <br>
                <li><b> Sep 2021, </b> <a href="https://xiaoman-zhang.github.io/Layer-Decomposition/"> Self-supervised Tumor Segmentation through Layer Decomposition.</a> <em>Preprint</em> <br> <br>
                <li><b> July 2021, </b> <a href="https://charigyang.github.io/motiongroup/"> Self-supervised Video Object Segmentation by Motion Grouping.</a>
                <em>ICCV2021</em>. &nbsp <font color="red"><strong>(Best Paper Award at CVPR Workshop)</strong></font> </a> <br> <br>
            </td>
          </tr>
          </table>
                  </td>
              </tr>
            </table>
          
          </body>
          
          </html>
